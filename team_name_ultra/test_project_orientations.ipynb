{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aux_fun'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ec827cf2ef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maux_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maux_fun\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_binned_spike_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_binned_spike_trains_sorted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msanity_check\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aux_fun'"
     ]
    }
   ],
   "source": [
    "#import all the stuff you need\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "from allensdk.brain_observatory.ecephys import ecephys_session\n",
    "\n",
    "# uncomment if you wanna use the tranfer entropy measure\n",
    "import sys\n",
    "testdir = os.getcwd() # os.path.dirname(__file__)\n",
    "import smite\n",
    "\n",
    "# fix slow autocomplete\n",
    "%config Completer.use_jedi = False\n",
    "%matplotlib inline\n",
    "\n",
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "if 'Darwin' in platstring:\n",
    "    # OS X \n",
    "    data_root = \"/Volumes/Brain2019/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn1' in platstring):\n",
    "    # then on AWS\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2019/\"\n",
    "    data_root = \"/run/media/tom.chartrand/Brain2019\"\n",
    "\n",
    "manifest_path = os.path.join(data_root, \"dynamic-brain-workshop/visual_coding_neuropixels/2019/manifest.json\")\n",
    "\n",
    " \n",
    "cache = EcephysProjectCache.fixed(manifest=manifest_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_binned_spike_trains() missing 2 required positional arguments: 'session_id' and 'stim_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1b9815ae6c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# from sanity_check import sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0maux_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_binned_spike_trains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m333\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_binned_spike_trains() missing 2 required positional arguments: 'session_id' and 'stim_type'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tools import aux_fun\n",
    "# from aux_fun import get_binned_spike_trains, get_binned_spike_trains_sorted\n",
    "# from sanity_check import sanity_check\n",
    "\n",
    "aux_fun.get_binned_spike_trains(333)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import swdb_2019_student as swdb\n",
    "# from  swdb_2019_tools import sanity_check\n",
    "# from sanity_check import sanity_check\n",
    "\n",
    "# from swdb_2019_student.sanity_check import sanity_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = cache.get_session_data(session_id)\n",
    "# my_units = session.units\n",
    "# sorted = my_units.sort_index(by='probe_vertical_position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(session.stimulus_conditions.stimulus_name.unique())\n",
    "\n",
    "\n",
    "session_id=737581020\n",
    "stim_type='drifting_gratings'#''#'spontaneous'natural_scenes''drifting_gratings\n",
    "time_step = 1/100\n",
    "\n",
    "areas1 = ['VISp']\n",
    "areas2 = ['VISp','VISl','VISal','VISrl','VISam','VISpm']\n",
    "\n",
    "T,units = get_binned_spike_trains_sorted(cache,session_id,stim_type,time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(T[:,:,:])\n",
    "#Ttransposed = T.transpose()\n",
    "#sanity_check(Ttransposed[:,:,:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a specific stim \n",
    "session = cache.get_session_data(session_id)\n",
    "stim_table = session.get_presentations_for_stimulus(stim_type)\n",
    "stim_ids = stim_table.index.values\n",
    "if stim_type == 'natural_scenes':\n",
    "    print('natural scenes')\n",
    "    stim_cond = stim_conditions = stim_table.stimulus_condition_id.values\n",
    "    unique_cond = stim_table.stimulus_condition_id.unique()\n",
    "    frames = session.get_stimulus_parameter_values(stimulus_presentation_ids=stim_ids, drop_nulls=False)\n",
    "elif stim_type == 'drifting_gratings':\n",
    "    print('drifting')\n",
    "    unique_cond = stim_table.orientation[stim_table.orientation != 'null'].unique()\n",
    "    stim_cond = stim_conditions = stim_table.orientation.values\n",
    "\n",
    "stim_table.orientation != 'null']\n",
    "\n",
    "\n",
    "\n",
    "# frames = np.sort(frames)\n",
    "# frames\n",
    "# frames\n",
    "#stim_table.orientation.unique()\n",
    "#print(frames)\n",
    "# unique_cond\n",
    "stim_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get stimulus presentation IDs for a given stimulus \n",
    "\n",
    "# stim_table = session.get_presentations_for_stimulus('drifting_gratings')\n",
    "\n",
    "#And drop blank presentations\n",
    "\n",
    "# #stim_table=stim_table[stim_table.orientation != 'null']\n",
    "# stim_types = len(stim_table.orientation.unique())\n",
    "\n",
    "# stim_orien\n",
    "\n",
    "    \n",
    "# stim_presentation_ids = stim_table[stim_table.orientation == stim_orien[0]].index.values\n",
    "\n",
    "# #     histograms = session.presentationwise_spike_counts(\n",
    "# #         bin_edges=bins,\n",
    "# #         stimulus_presentation_ids=stim_presentation_ids,\n",
    "# #         unit_ids=sample_units.index.values\n",
    "# #     )\n",
    "# print(len(stim_presentation_ids))\n",
    "# print(75*8)\n",
    "# T.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(stim_table)\n",
    "# len(stim_ids)\n",
    "# unique_stims = np.unique(stim_ids)\n",
    "# len(stim_conditions)\n",
    "# stim_conditions\n",
    "# T.shape[0]\n",
    "# print(stim_conditions)\n",
    "# print(stim_ids)\n",
    "# all_ids = stim_conditions = stim_table.stimulus_condition_id.values\n",
    "# print(all_ids)\n",
    "# 5950/len(unique_cond)\n",
    "# T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each stimulus in frames average across trials \n",
    "num_stims = len(unique_cond)\n",
    "Avg_psth = np.zeros((T.shape[0],T.shape[1],num_stims))\n",
    "for ix,cond in enumerate(unique_cond):\n",
    "    #print(ix)\n",
    "    idx_in = stim_cond == cond\n",
    "    #print(sum(idx_in))\n",
    "    Avg_psth[:,:,ix] = np.nanmean(T[:,:,idx_in],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.nanmean(T[:,:,idx_in],2)\n",
    "#Ttranp = np.transpose(Avg_psth)\n",
    "#sanity_check(Avg_psth[:,:,:10])\n",
    "#sanity_check(Ttranp[:,:,:16],ncols=8)\n",
    "\n",
    "#print(Avg_psth.shape)\n",
    "#print(TT.shape)\n",
    "\n",
    "\n",
    "# units.head()\n",
    "# units.structure_acronym.unique()\n",
    "Avg_psth.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units['Row_number'] = np.arange(len(units))\n",
    "unitIDs1 = []\n",
    "unitIDs2 = []\n",
    "for index in units.index:\n",
    "    structure_acronym = units.structure_acronym[index]\n",
    "#     print(index, structure_acronym)\n",
    "    if structure_acronym in areas1:\n",
    "        unitIDs1.append(index)    \n",
    "    if structure_acronym in areas2:\n",
    "        unitIDs2.append(index)    \n",
    "\n",
    "unit_ind1 = units.Row_number[unitIDs1].values\n",
    "unit_ind2 = units.Row_number[unitIDs2].values\n",
    "\n",
    "N1 = len(unitIDs1)\n",
    "N2 = len(unitIDs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(T.shape)\n",
    "# T[0,1:10,1]\n",
    "# T.shape[0]\n",
    "#unit_ind1\n",
    "print(N1)\n",
    "print(N2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = Avg_psth\n",
    "import scipy.stats as stats\n",
    "import statsmodels as statsmodels\n",
    "from statsmodels import tsa\n",
    "from statsmodels.tsa import stattools\n",
    "corr_mat = np.zeros([N1,N2,np.minimum(10,XX.shape[2])])\n",
    "for itrial in range(np.minimum(10,XX.shape[2])):\n",
    "    print(itrial)\n",
    "#itrial=2\n",
    "    for i in range(N1):\n",
    "        for j in range(N2):\n",
    "#             corr_mat[i,j,itrial] = stats.pearsonr(XX[unit_ind1[i],:,itrial],XX[unit_ind2[j],:,itrial])[0]\n",
    "            #print(j)\n",
    "\n",
    "            # uncomment this for granger casuality\n",
    "            #zipped_array  = np.array(list(zip(TT[i,:,itrial],TT[j,:,itrial])))\n",
    "            #granger_test_result = statsmodels.tsa.stattools.grangercausalitytests(zipped_array,maxlag=10);\n",
    "            #corr_mat[i,j,itrial]=granger_test_result[10][0]['ssr_ftest'][0]\n",
    "            \n",
    "            symX = smite.symbolize(XX[unit_ind1[i],:,itrial],3)\n",
    "            symY = smite.symbolize(XX[unit_ind2[j],:,itrial],3)\n",
    "\n",
    "            TXY = smite.symbolic_transfer_entropy(symX, symY)\n",
    "            TYX = smite.symbolic_transfer_entropy(symY, symX)\n",
    "            corr_mat[i,j,itrial]=TYX - TXY\n",
    "            #corr_mat[j,i,itrial]=TXY - TYX   \n",
    "            \n",
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "     with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_object(corr_mat, str('corr_mat_' + stim_type + 'session_' + str(session_id) +'.pkl'))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(corr_mat,'units 1','units 2','stimulus',ncols=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_num = int(stim_table.frame[stim_table.stimulus_condition_id == unique_cond[4]].unique())\n",
    "# image_template = cache.get_natural_scene_template(image_num)\n",
    "\n",
    "# plt.imshow(image_template, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(corr_mat[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_\n",
    "print(pca.explained_variance_ratio_)\n",
    "pca.components_ #eigenvectors\n",
    "X_project = pca.fit_transform(corr_mat[:,:,0])\n",
    "max_pc = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_project.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.components_[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_id = np.argsort(X_project[:,0])\n",
    "sanity_check(corr_mat[sort_id,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(corr_mat[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "\n",
    "kmeans = KMeans(n_clusters=3).fit(X_project[:,:3])\n",
    "unsupervised_y_hat = kmeans.labels_\n",
    "\n",
    "# class_0, __ = stats.mode(unsupervised_y_hat[y==0])\n",
    "# class_1, __ = stats.mode(unsupervised_y_hat[y==1])\n",
    "# class_2, __ = stats.mode(unsupervised_y_hat[y==2])\n",
    "\n",
    "# sorted_unsup_y_hat = unsupervised_y_hat.copy()\n",
    "# sorted_unsup_y_hat[unsupervised_y_hat==class_0] = 0\n",
    "# sorted_unsup_y_hat[unsupervised_y_hat==class_1] = 1\n",
    "# sorted_unsup_y_hat[unsupervised_y_hat==class_2] = 2\n",
    "\n",
    "# plot_test_performance(X_project,y,sorted_unsup_y_hat)\n",
    "plt.scatter(X_project[unsupervised_y_hat==0,0],X_project[unsupervised_y_hat==0,1])\n",
    "plt.scatter(X_project[unsupervised_y_hat==1,0],X_project[unsupervised_y_hat==1,1])\n",
    "plt.scatter(X_project[unsupervised_y_hat==2,0],X_project[unsupervised_y_hat==2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_id = np.argsort(unsupervised_y_hat)\n",
    "sanity_check(corr_mat[sort_id,:,0])\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# lfp_plot = lfp.loc[dict(time=slice(5,20))]\n",
    "# x, y = lfp_plot.time, range(len(lfp_plot.channel))\n",
    "# plt.pcolormesh(x, y, lfp_plot.values.T)\n",
    "# plt.colorbar(ax=ax)\n",
    "\n",
    "# ax.set_xlabel(\"time (s)\")\n",
    "\n",
    "# # include the structure data\n",
    "# structure_acronyms, intervals = session.channel_structure_intervals(lfp.channel.values)\n",
    "# ax.set_yticks(intervals)\n",
    "# interval_midpoints = [ (aa + bb) / 2 for aa, bb in zip(intervals[:-1], intervals[1:])]\n",
    "# ax.set_yticks(interval_midpoints, minor=True)\n",
    "# ax.set_yticklabels(structure_acronyms, minor=True)\n",
    "# plt.tick_params(\"y\", which=\"major\", labelleft=False, length=40)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "units_V1 = units.loc[unitIDs1]\n",
    "units_V1['cluster']=unsupervised_y_hat\n",
    "\n",
    "save_object(units_V1, str('V1_clusters' + stim_type + 'session_' + str(session_id) +'.pkl'))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scary_function(a, b):\n",
    "#     return a + b\n",
    "\n",
    "# print(scary_function(0, 1))\n",
    "# print(scary_function(\"a\", \"b\"))\n",
    "# print(scary_function(np.array([1, 2, 3]), np.array([4, 5, 6])) )\n",
    "\n",
    "# aa= 'a', 4\n",
    "# print(aa, type(aa))\n",
    "\n",
    "# bb= [np.array([1, 2, 3]), np.array([4, 5, 6]), \"hi\"]\n",
    "# print(bb, type(bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #zipped_array = np.array(list(zip(TT[0,:,0],TT[1,:,0])))\n",
    "# x1 = np.random.rand(100)\n",
    "# y1 = np.concatenate([x1[3:], x1[:3]])\n",
    "# print(x1.shape)\n",
    "\n",
    "# print(y1.shape)\n",
    "# zipped_array  = np.array(list(zip(x1,y1)))\n",
    "# zipped_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Statistical tools for time series analysis\n",
    "# \"\"\"\n",
    "# import statsmodels as statsmodels\n",
    "# from statsmodels import tsa\n",
    "# from statsmodels.tsa import stattools\n",
    "\n",
    "# granger_test_result = statsmodels.tsa.stattools.grangercausalitytests(zipped_array,maxlag=15);\n",
    "# print(granger_test_result[key][0])\n",
    "# #granger_test_result = grangercausalitytests(data[:, 1::-1], maxlag=12, verbose#=False)\n",
    "                                            \n",
    "# # optimal_lag = -1\n",
    "# # F_test = -1.0\n",
    "# # for key in granger_test_result.keys():\n",
    "# #      _F_test_ = granger_test_result[key][0]['params_ftest'][0]\n",
    "# # #     if _F_test_ > F_test:\n",
    "# #         F_test = _F_test_\n",
    "# #         optimal_lag = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(granger_test_result[1][0]['ssr_ftest'])\n",
    "\n",
    "# print(type(granger_test_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
